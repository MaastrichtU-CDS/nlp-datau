{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calculate NLP statistics over classification results in excel format\n",
    "Analyse a excel sheet of NLP classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use yaml config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "use_config_file = True\n",
    "config_file = './../resources/config/result-analysis-tn-tn-train.yaml'\n",
    "cfg = None\n",
    "if use_config_file:\n",
    "    import yaml\n",
    "    with open(config_file, 'r') as yaml_file:\n",
    "        cfg = yaml.safe_load(yaml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set path of excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_excel_sheet = \"./../resources/data_ignored/results-tn-train.xlsx\"\n",
    "if use_config_file:\n",
    "    path_excel_sheet = cfg['path_excel_sheet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_output_dir = \"./../resources/\"\n",
    "\n",
    "out_file = None\n",
    "out_file_excel = None\n",
    "if use_config_file:\n",
    "    path_output_dir = cfg['path_output_dir']\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    Path(path_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    out_file = open(path_output_dir + \"/\"+ \"result-analysis-out.txt\", \"w\")\n",
    "    out_file_excel = path_output_dir + \"/\"+ \"result-analysis-out.xlsx\"\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path_output_dir)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path_output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set label and classification columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_actual = 'label longembolie'\n",
    "column_predicted = 'target-snomedct:59282003'\n",
    "\n",
    "if use_config_file:\n",
    "    column_actual = cfg['column_actual']\n",
    "    column_predicted = cfg['column_predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read columns and determine values, in case of key error, make sure the columns are set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.read_excel(path_excel_sheet, header=0)\n",
    "\n",
    "actual_values = df[column_actual].value_counts().index.tolist() \n",
    "predicted_values = df[column_predicted].value_counts().index.tolist() \n",
    "\n",
    "print('actual category values:', actual_values)\n",
    "print('predicted category values:', predicted_values)\n",
    "all_values = list(set(actual_values).union(set(predicted_values)))\n",
    "print(all_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Manually set valid values from actual and predicted categories (other values will be ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "valid_values = []\n",
    "ignored_values = []\n",
    "\n",
    "if use_config_file and 'valid_values' in cfg:\n",
    "    valid_values = cfg['valid_values']\n",
    "if use_config_file and 'ignored_values' in cfg:\n",
    "    ignored_values = cfg['ignored_values']\n",
    "\n",
    "if not valid_values or len(valid_values) == 0:\n",
    "    valid_values = all_values\n",
    "    \n",
    "def matches_ignored(x):\n",
    "    for ignored in ignored_values:\n",
    "        if ignored in x:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "valid_values = [] \n",
    "for x in all_values:\n",
    "    if not matches_ignored(x):\n",
    "        valid_values.append(x)\n",
    "\n",
    "valid_values.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_valid = df[df[column_actual].isin(valid_values)]\n",
    "\n",
    "actual_index_valid = df_valid[column_actual].value_counts().index.tolist() \n",
    "predicted_index_valid = df_valid[column_predicted].value_counts().index.tolist() \n",
    "\n",
    "print('actual category values:', actual_index_valid)\n",
    "print('predicted category values:', predicted_index_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Histograms Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual_bar_plt = df[column_actual].value_counts(sort = False).plot(kind='bar', title='actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Histograms Valid Values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual_valid_bar_plot = df_valid[column_actual].value_counts().plot(kind='bar', title='actual valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_valid_bar_plt = df_valid[column_predicted].value_counts().plot(kind='bar', title='predicted valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df_valid\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "y_actu = pandas.Series(df[column_actual], name='Actual')\n",
    "y_pred = pandas.Series(df[column_predicted], name='Predicted')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "accuracy = \"accuracy = \" + str(round(accuracy_score(y_actu, y_pred), 4))\n",
    "macro_precision = \"macro_precision = \" + str(round(precision_score(y_actu, y_pred, average=\"macro\"), 4))\n",
    "macro_recall_score = \"macro_recall_score = \" + str(round(recall_score(y_actu, y_pred, average=\"macro\"), 4))\n",
    "macro_f1 = \"macro_f1 = \" + str(round(f1_score(y_actu, y_pred, average=\"macro\"), 4))\n",
    "\n",
    "print(accuracy)\n",
    "print(macro_precision)\n",
    "print(macro_recall_score)\n",
    "print(macro_f1)       \n",
    "\n",
    "out_file.write(accuracy + \"\\n\\n\")\n",
    "out_file.write(macro_precision + \"\\n\\n\")\n",
    "out_file.write(macro_recall_score + \"\\n\\n\")\n",
    "out_file.write(macro_f1 + \"\\n\\n\")\n",
    "\n",
    "labels = unique_labels(y_actu, y_pred)\n",
    "\n",
    "prf = precision_recall_fscore_support(y_actu, y_pred, average=None, labels=valid_values)\n",
    "evaldf = pandas.DataFrame({\"Precision\": prf[0], \"Recall\": prf[1], \"F-score\": prf[2]}, index=valid_values)\n",
    "print(evaldf)\n",
    "out_file.write(str(evaldf) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "crosstab = pandas.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(crosstab)\n",
    "out_file.write(str(crosstab))\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "crosstab.to_excel(out_file_excel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'y_Actual': df_valid[column_actual], 'y_Predicted': df_valid[column_predicted]}\n",
    "df = pandas.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "\n",
    "confusion_matrix = pandas.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "plt.figure(figsize=(16,16))  \n",
    "plt.title(cfg['title_confusion_matrix'])\n",
    "\n",
    "confusion_matrix_heatmap = seaborn.heatmap(confusion_matrix, annot=True, cmap=matplotlib.cm.Blues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save confusion matix figure to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_type_eps = False\n",
    "fig = confusion_matrix_heatmap.get_figure()\n",
    "if file_type_eps:\n",
    "    fig.savefig(path_output_dir + \"/\" + \"confusion_matrix.eps\", dict=\"eps\", dpi=600)\n",
    "else:\n",
    "   fig.savefig(path_output_dir + \"/\" + \"confusion_matrix.png\", dpi=600)\n",
    "\n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
