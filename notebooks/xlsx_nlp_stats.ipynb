{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calculate NLP statistics over classification results in excel format\n",
    "Analyse a excel sheet of NLP classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use yaml config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "use_config_file = True\n",
    "# config_file = './../resources/config/result-analysis/tnm/result-analysis-tn-val.yaml'\n",
    "config_file = './../resources/config/result-analysis/concept/result-analysis-concept1.yaml'\n",
    "\n",
    "cfg = None\n",
    "if use_config_file:\n",
    "    import yaml\n",
    "    with open(config_file, 'r') as yaml_file:\n",
    "        cfg = yaml.safe_load(yaml_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set path of excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_excel_sheet = \"./../resources/data_ignored/Comparison with ground truth.xlsx\"\n",
    "if use_config_file:\n",
    "    path_excel_sheet = cfg['path_excel_sheet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_output_dir = \"./../resources/\"\n",
    "\n",
    "out_file = None\n",
    "out_file_excel = None\n",
    "if use_config_file:\n",
    "    path_output_dir = cfg['path_output_dir']\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    Path(path_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    out_file = open(path_output_dir + \"/\"+ \"result-analysis-out.txt\", \"w\")\n",
    "    out_file_excel = path_output_dir + \"/\"+ \"result-analysis-out.xlsx\"\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path_output_dir)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path_output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set label and classification columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_actual = 'label longembolie'\n",
    "column_predicted = 'target-snomedct:59282003'\n",
    "\n",
    "if use_config_file:\n",
    "    column_actual = cfg['column_actual']\n",
    "    column_predicted = cfg['column_predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read columns and determine values, in case of key error, make sure the columns are set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.read_excel(path_excel_sheet, header=0)\n",
    "\n",
    "def replace_by_bool():\n",
    "    df[column_actual] = df[column_actual].astype(bool)\n",
    "    df[column_predicted] = df[column_predicted].astype(bool)\n",
    "\n",
    "\n",
    "def remove_invalid_labels(): \n",
    "    df = df[df[column_actual] != \"?\"] # Filter dataframe on reports that have a valid label\n",
    "    df = df[df[column_actual] != \"?\"] # Filter dataframe on reports that have a valid label\n",
    "\n",
    "    \n",
    "replace_bool = False # this should be configurable\n",
    "remove_labels = False\n",
    "if replace_bool: replace_by_bool()\n",
    "if remove_labels: remove_invalid_labels()\n",
    "\n",
    "    \n",
    "actual_values = df[column_actual].value_counts().index.tolist() \n",
    "predicted_values = df[column_predicted].value_counts().index.tolist() \n",
    "all_values = list(set(actual_values).union(set(predicted_values)))\n",
    "\n",
    "print('actual category values={}'.format(actual_values))\n",
    "print('predicted category values={}'.format(predicted_values))\n",
    "print('all values={}'.format(all_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Manually set valid values from actual and predicted categories (other values will be ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "valid_values = []\n",
    "ignored_values = []\n",
    "\n",
    "if use_config_file and 'valid_values' in cfg:\n",
    "    valid_values = cfg['valid_values']\n",
    "if use_config_file and 'ignored_values' in cfg:\n",
    "    ignored_values = cfg['ignored_values']\n",
    "\n",
    "if not valid_values or len(valid_values) == 0:\n",
    "    valid_values = all_values\n",
    "    \n",
    "def matches_ignored(value):\n",
    "    for ignored in ignored_values:\n",
    "        if ignored == value:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "valid_values = [] \n",
    "for value in all_values:\n",
    "    if not matches_ignored(value):\n",
    "        valid_values.append(value)\n",
    "\n",
    "valid_values.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_valid = df[df[column_actual].isin(valid_values)]\n",
    "\n",
    "actual_index_valid = df_valid[column_actual].value_counts().index.tolist() \n",
    "predicted_index_valid = df_valid[column_predicted].value_counts().index.tolist() \n",
    "\n",
    "print('actual category values:', actual_index_valid)\n",
    "print('predicted category values:', predicted_index_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Histograms Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual_bar_plt = df[column_actual].value_counts(sort = False).plot(kind='bar', title='actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Histograms Valid Values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual_valid_bar_plot = df_valid[column_actual].value_counts().plot(kind='bar', title='actual valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted_valid_bar_plt = df_valid[column_predicted].value_counts().plot(kind='bar', title='predicted valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df_valid\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "y_actu = pandas.Series(df[column_actual], name='Actual')\n",
    "y_pred = pandas.Series(df[column_predicted], name='Predicted')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "accuracy = \"accuracy = \" + str(round(accuracy_score(y_actu, y_pred), 4))\n",
    "print(accuracy)\n",
    "out_file.write(accuracy + \"\\n\\n\")\n",
    "\n",
    "avg = [\"micro\", \"macro\", \"weighted\"]\n",
    "for avg_opt in avg:\n",
    "    a_precision = avg_opt + \"_precision = \" + str(round(precision_score(y_actu, y_pred, average=avg_opt), 4))\n",
    "    a_recall_score = avg_opt + \"_recall_score = \" + str(round(recall_score(y_actu, y_pred, average=avg_opt), 4))\n",
    "    a_f1 = avg_opt + \"_f1 = \" + str(round(f1_score(y_actu, y_pred, average=avg_opt), 4))\n",
    "    print(a_precision)\n",
    "    print(a_recall_score)\n",
    "    print(a_f1)       \n",
    "    out_file.write(a_precision + \"\\n\")\n",
    "    out_file.write(a_recall_score + \"\\n\")\n",
    "    out_file.write(a_f1 + \"\\n\")\n",
    "    out_file.write(\"\\n\")\n",
    "    \n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "predicted = [1,2,3,4,5,1,2,1,1,4,5] \n",
    "\n",
    "precision, recall, fscore, support = score(y_actu, y_pred)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "out_file.write('precision: {}'.format(precision) + '\\n')\n",
    "out_file.write('recall: {}'.format(recall) +'\\n')\n",
    "out_file.write('fscore: {}'.format(fscore) + '\\n')\n",
    "out_file.write('support: {}'.format(support) + '\\n')\n",
    "print('\\n')\n",
    "               \n",
    "prf = precision_recall_fscore_support(y_actu, y_pred, average=None, labels=valid_values)\n",
    "evaldf = pandas.DataFrame({\"Precision\": prf[0], \"Recall\": prf[1], \"F-score\": prf[2]}, index=valid_values)\n",
    "print(evaldf)\n",
    "out_file.write(str(evaldf) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "crosstab = pandas.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(crosstab)\n",
    "out_file.write(str(crosstab))\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "crosstab.to_excel(out_file_excel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'y_Actual': df_valid[column_actual], 'y_Predicted': df_valid[column_predicted]}\n",
    "df = pandas.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "\n",
    "confusion_matrix = pandas.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "plt.figure(figsize=(cfg['confusion_matrix']['fig_size']['x'],cfg['confusion_matrix']['fig_size']['y']))  \n",
    "plt.title(cfg['confusion_matrix']['title'] + '\\n')\n",
    "\n",
    "seaborn.set(font_scale=1)\n",
    "\n",
    "confusion_matrix_heatmap = seaborn.heatmap(confusion_matrix, annot=True, cmap=matplotlib.cm.Blues, fmt='d' )\n",
    "confusion_matrix_heatmap.set_xticklabels(confusion_matrix_heatmap.get_xmajorticklabels(), fontsize=cfg['confusion_matrix']['font_size'])\n",
    "\n",
    "confusion_matrix_heatmap.axes.set_title(confusion_matrix_heatmap.get_title(),fontsize=cfg['confusion_matrix']['font_size'])\n",
    "confusion_matrix_heatmap.set_xlabel(confusion_matrix_heatmap.get_xlabel(),fontsize=cfg['confusion_matrix']['font_size'], labelpad=20)\n",
    "confusion_matrix_heatmap.set_ylabel(confusion_matrix_heatmap.get_ylabel(),fontsize=cfg['confusion_matrix']['font_size'], labelpad=20)\n",
    "confusion_matrix_heatmap.tick_params(labelsize=cfg['confusion_matrix']['tick_label_font_size'])\n",
    "confusion_matrix_heatmap.set_yticklabels(confusion_matrix_heatmap.get_ymajorticklabels(),  rotation = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save confusion matix figure to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = confusion_matrix_heatmap.get_figure()\n",
    "fig.savefig(path_output_dir + '/' + cfg['confusion_matrix']['title'].lower().replace(' ','_') + '.eps', bbox_inches='tight', dict=\"eps\", dpi=600)\n",
    "fig.savefig(path_output_dir + '/' + cfg['confusion_matrix']['title'].lower().replace(' ','_') + '.png', bbox_inches='tight', dpi=200)\n",
    "\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
